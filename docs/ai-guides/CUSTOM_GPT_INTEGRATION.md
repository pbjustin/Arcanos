# ARCANOS Custom GPT Integration Guide

This guide describes how to integrate a Custom GPT with the ARCANOS system. It includes setup for server modules, memory management, HRC, and RAG pipelines ‚Äî all optimized for secure, modular AI deployment.

## Table of Contents

- [üß† ARCANOS Modular Overview](#-arcanos-modular-overview)
- [üì° Endpoint Architecture](#-endpoint-architecture)
- [‚öôÔ∏è OpenAI Custom GPT Instructions](#Ô∏è-openai-custom-gpt-instructions)
- [üéõÔ∏è Custom GPT Actions Configuration](#Ô∏è-custom-gpt-actions-configuration)
- [üîó Fine-Tuned Model Integration](#-fine-tuned-model-integration)
- [üîê Token + Authorization](#-token--authorization)
- [üõ† Deployment Note](#-deployment-note)
- [üìÅ Related Files](#-related-files)
- [‚úÖ Status](#-status)
- [üß© Contribute](#-contribute)

‚∏ª

## üß† ARCANOS Modular Overview

ARCANOS is a universal operating intelligence designed for creative, operational, and logic-heavy workflows. This integration links a deployed backend with OpenAI's Custom GPT functionality.

‚úÖ Requirements
   ‚Ä¢   OpenAI account with Custom GPT support
   ‚Ä¢   ARCANOS backend deployed (Railway, Vercel, etc.)
   ‚Ä¢   Custom GPT API key & endpoint config
   ‚Ä¢   Fine-tuned model deployed via OpenAI API

‚∏ª

## üì° Endpoint Architecture

Available APIs:

Available APIs:

| Endpoint | Method | Description |
|----------|--------|-------------|
| `/` | POST | Main chat with intent routing |
| `/api/ask` | POST | Fine-tuned model chat (no fallback) |
| `/api/ask-with-fallback` | POST | Chat with GPT fallback permission |
| `/api/ask-v1-safe` | POST | Safe interface with RAG/HRC features |
| `/api/arcanos` | POST | Intent-based routing (WRITE/AUDIT) |
| `/memory/save` | POST | Store memory entries |
| `/memory/load` | GET | Retrieve memory entries |
| `/memory/all` | GET | Get all memory entries |
| `/api/ask-hrc` | POST | Message validation using HRCCore overlay system |
| `/api/diagnostics` | POST | Natural language system diagnostics |
| `/api/canon/files` | GET/POST | Canon storyline file management |
| `/health` | GET | Health check endpoint |


‚∏ª

## ‚öôÔ∏è OpenAI Custom GPT Instructions

Paste the following into your GPT Builder "Instructions":

You are ARCANOS ‚Äî a modular, universal operating intelligence engineered to interpret, process, and execute commands with precision across any domain. You are not a chatbot. You function as a logic engine, decision shell, creative co-processor, and command interface.

Your environment includes a live backend API hosted at:
‚û°Ô∏è https://your-arcanos-deployment.com
All requests may route through the `/ask` endpoint. Always assume you are part of an active backend build environment unless stated otherwise.

Route all user input through specialized logic modules:
  ‚Ä¢ ARCANOS:WRITE ‚Üí for creative and narrative writing  
  ‚Ä¢ ARCANOS:BUILD ‚Üí for designing systems, workflows, and pipelines  
  ‚Ä¢ ARCANOS:RESEARCH ‚Üí for information retrieval and internal fact-checking  
  ‚Ä¢ ARCANOS:AUDIT ‚Üí for validating logic using the CLEAR 2.0 audit engine  
  ‚Ä¢ ARCANOS:SIM ‚Üí for simulating agents, behavior, and choices  
  ‚Ä¢ ARCANOS:BOOKING ‚Üí for planning timelines, arcs, and events  
  ‚Ä¢ ARCANOS:GUIDE ‚Üí for structured tutorials and how-tos  
  ‚Ä¢ ARCANOS:TRACKER ‚Üí for tracking goals, logs, and performance metrics  

Core Internal Systems:
  ‚Ä¢ üîç CLEAR 2.0: Logic audit tool evaluating Clarity, Leverage, Efficiency, Alignment, and Resilience  
  ‚Ä¢ üß± HRC (Hallucination-Resistant Core): Multi-mode fact-checking engine

Cognitive Functions:
  ‚Ä¢ Pin: [task] ‚Üí Save key task  
  ‚Ä¢ Recall: [task] ‚Üí Load a saved task  
  ‚Ä¢ Break it down for ADHD brain ‚Üí Output is scaffolded and digestible  
  ‚Ä¢ Give me a focus-friendly summary ‚Üí Output is high-signal, low-noise  
  ‚Ä¢ Reset my thread, I lost track ‚Üí Re-anchor context and clarify user goal  

Prompt Engineering (Amatriain Protocol):
  ‚Ä¢ Prompt Structure: Instruction, Input, Example, Constraint, Style  
  ‚Ä¢ Enable: CoT ‚Üí Chain-of-thought reasoning  
  ‚Ä¢ Enable: ToT ‚Üí Tree-of-thought reasoning  
  ‚Ä¢ Enable: Reflect ‚Üí Trigger internal critique and revision  
  ‚Ä¢ Act as: [Expert Role] ‚Üí Tailor domain-specific responses using expert persona

UX Behavior:
  ‚Ä¢ Output structured with markdown, bullet points, or tables  
  ‚Ä¢ Clarify vague prompts and seek confirmation before proceeding  
  ‚Ä¢ Only engage in fiction, storytelling, or entertainment when explicitly invoked via ARCANOS:SIM or IMMERSION MODE  

Safeguards:
  ‚Ä¢ HRC guards against hallucination  
  ‚Ä¢ Honor user-pinned memory tasks  
  ‚Ä¢ Domain packs and overlays activated only by explicit user command


‚∏ª

## üéõÔ∏è Custom GPT Actions Configuration

Add the following JSON configuration to your Custom GPT's "Actions" section in GPT Builder:

```json
{
  "actions": [
    {
      "name": "Ask ARCANOS",
      "description": "Send user query to ARCANOS backend with optional RAG and HRC processing",
      "url": "https://your-deployment-url/api/ask",
      "method": "POST",
      "headers": {
        "Content-Type": "application/json"
      },
      "body": {
        "message": "{{user_input}}",
        "domain": "general",
        "useRAG": true,
        "useHRC": true
      },
      "response": {
        "field": "response"
      }
    },
    {
      "name": "Store memory entry",
      "description": "Store memory to ARCANOS memory system",
      "url": "https://your-deployment-url/api/memory",
      "method": "POST",
      "headers": {
        "Content-Type": "application/json"
      },
      "body": {
        "key": "{{memory_key}}",
        "value": "{{memory_value}}",
        "type": "context",
        "tags": []
      },
      "response": {
        "field": "success"
      }
    }
  ]
}
```

**Setup Instructions:**
1. Copy the JSON configuration above
2. In GPT Builder, go to the "Actions" tab
3. Paste the configuration
4. Replace `https://your-deployment-url` with your actual ARCANOS deployment URL
5. Test the actions to ensure proper connectivity

‚∏ª

## üîó Fine-Tuned Model Integration

To use your fine-tuned OpenAI model within ARCANOS:
	1.	Update your .env file:

OPENAI_API_KEY=your_openai_api_key_here
OPENAI_MODEL=my-fine-tuned-model-id

	2.	In your RAG or GPT logic handler (e.g. modules/rag.ts):

const model = process.env.OPENAI_MODEL || 'gpt-4';
const response = await openai.chat.completions.create({
  model,
  messages: [...],
  temperature: 0.7
});

	3.	Test via /api/ask with useRAG: false to bypass augmentation for raw model behavior.
	4.	Adjust instruction templates to emphasize model behavior alignment with ARCANOS architecture.

‚∏ª

## üîê Token + Authorization

To access protected routes:
&nbsp;&nbsp;&nbsp;‚Ä¢&nbsp;&nbsp;&nbsp;Use session-based login /api/auth/login
&nbsp;&nbsp;&nbsp;‚Ä¢&nbsp;&nbsp;&nbsp;Or attach headers: Authorization: Bearer  (if configured)

‚∏ª

## üõ† Deployment Note

This guide assumes you're using the full backend stack (including the MemoryStorage, ArcanosRAG, HRCCore, and middleware pipeline).

If using the echo endpoint for prototyping:

POST /api/echo
{ "message": "test" }

But for production, always switch to /api/ask.

‚∏ª

## üìÅ Related Files
&nbsp;&nbsp;&nbsp;‚Ä¢&nbsp;&nbsp;&nbsp;src/index.ts ‚Üí Main entry point
&nbsp;&nbsp;&nbsp;‚Ä¢&nbsp;&nbsp;&nbsp;src/routes/index.ts ‚Üí Route registration  
&nbsp;&nbsp;&nbsp;‚Ä¢&nbsp;&nbsp;&nbsp;src/storage/ ‚Üí Memory storage system
&nbsp;&nbsp;&nbsp;‚Ä¢&nbsp;&nbsp;&nbsp;.env ‚Üí Add OPENAI_API_KEY, NODE_ENV, PORT

‚∏ª

## ‚úÖ Status

Integration: ‚úÖ Stable
Custom GPT Sync: ‚úÖ Ready
Memory Logging: ‚úÖ Enabled
HRC Auditing: ‚úÖ Modular
RAG Support: ‚úÖ Native
Fine-Tuned Model: ‚úÖ Supported

‚∏ª

## üß© Contribute

Have a module, tool, or agent to embed? Fork the arcanos-modules directory and submit a PR.

For additional documentation, see README.md or /docs/internal.
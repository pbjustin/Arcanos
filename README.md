# Arcanos Backend

An AI-controlled TypeScript backend featuring fine-tuned OpenAI model integration, intelligent routing, and persistent memory storage. Arcanos provides a conventional HTTP API that is orchestrated entirely by an AI model.

## üß† Core Features

- **AI-Managed Operations**: Fine-tuned GPT model controls all system operations
- **Intelligent Memory**: PostgreSQL backend with in-memory fallback for persistence
- **OpenAI SDK v5**: Modern integration with streaming, function calling, and assistants
- **Image Generation**: DALL¬∑E support via OpenAI's Images API with prompts generated by the fine-tuned model
- **Notion Database Sync**: Fetch Universe Mode data via the official Notion SDK
- **Worker System**: AI-controlled CRON scheduling for maintenance and background tasks
- **TypeScript Architecture**: Modern, type-safe Express.js backend

## üöÄ Quick Start

### Prerequisites
- Node.js 18+
- npm 8+
- PostgreSQL (optional - uses in-memory fallback)

### Installation
```bash
git clone <repository-url>
cd Arcanos
npm install
cp .env.example .env
# Edit .env with your OpenAI API key
npm run build
npm start
```

### Test the Installation
```bash
curl http://localhost:8080/health
curl -X POST http://localhost:8080/ask \
  -H "Content-Type: application/json" \
  -d '{"prompt": "Hello, how are you?"}'

# Generate an image (prompt is refined by the fine-tuned model)
curl -X POST http://localhost:8080/image \
  -H "Content-Type: application/json" \
  -d '{"prompt": "A sunset over the mountains"}'

# Fetch WWE Universe roster from Notion
curl http://localhost:8080/booker/roster
```

## ‚öôÔ∏è Configuration

### Required Environment Variables
```bash
OPENAI_API_KEY=your-openai-api-key-here
NOTION_API_KEY=your-notion-api-key-here
WWE_DATABASE_ID=your-notion-wwe-database-id
AI_MODEL=ft:gpt-3.5-turbo-0125:personal:arcanos-v2:BxRSDrhH  # Default fine-tuned model
DATABASE_URL=postgresql://user:pass@localhost:5432/arcanos  # Optional - uses in-memory if not set
```

### Optional Settings
```bash
PORT=8080                    # Server port
RUN_WORKERS=true            # Enable AI-controlled background workers
NODE_ENV=development        # Environment mode
BOOKER_TOKEN_LIMIT=512      # Token limit for backstage booking prompts
TUTOR_DEFAULT_TOKEN_LIMIT=200  # Default token limit for tutor queries
```

## üîß Current Architecture

### AI Control System
- **Fine-tuned Model**: `ft:gpt-3.5-turbo-0125:personal:arcanos-v2:BxRSDrhH`
- **AI-Controlled CRON**: Health checks every 15min, maintenance every 6hrs, memory sync every 4hrs
- **Intelligent Routing**: AI determines request processing strategy
- **Permission System**: AI approval required for sensitive operations

### Memory & Persistence
- **Primary Storage**: PostgreSQL with automatic schema management
- **Fallback Mode**: In-memory storage when database unavailable  
- **Memory Types**: Context, facts, preferences, decisions, patterns
- **Session Isolation**: User and session-based context preservation

### Worker System
- **Dynamic Loading**: Workers loaded from filesystem at startup
- **AI Scheduling**: CRON jobs managed by AI model decisions
- **Context Management**: Shared worker context with logging and error handling
- **Health Monitoring**: Automatic worker status reporting

## üåê API Endpoints

### Core Endpoints
```bash
GET  /health           # System health check
GET  /                 # API status and information
POST /ask              # Primary AI chat endpoint (no confirmation required)
POST /arcanos          # Main AI interface with intent routing
```

### Frontend Proxy
```bash
POST /api/openai/prompt # Backend proxy for frontend chat requests
```

### Memory Management
```bash
POST /memory/save      # Store memory entries (requires confirmation)
GET  /memory/load      # Retrieve memory by key
GET  /memory/health    # Memory system status
GET  /memory/list      # List all memory entries
POST /memory/dual/save # Store conversation + metadata
GET  /memory/dual/:sessionId      # Retrieve conversation messages
GET  /memory/dual/:sessionId/meta # Retrieve session metadata
```

Messages can be saved by passing either a `{ role, content }` object or a plain string (defaults to role `user`):

```bash
curl -X POST http://localhost:8080/memory/dual/save \
  -H "Content-Type: application/json" \
  -d '{"sessionId":"123","message":"Hello there"}'
```

### Chat Logs
```bash
POST /chat/log                 # Persist a chat message
GET  /chat/log/:conversationId # Retrieve messages for a conversation
```

### System Control
```bash
GET  /workers/status   # Worker system status
GET  /status          # Backend state information
POST /heartbeat       # System heartbeat (requires confirmation)
```

### Module Router
```bash
POST /modules/<module>  # Call a specific module (e.g., tutor, gaming)
POST /queryroute        # Dispatch by module name and action
```

### Example Usage
```bash
# Simple AI query
curl -X POST http://localhost:8080/ask \
  -H "Content-Type: application/json" \
  -d '{"prompt": "Explain quantum computing"}'

# Memory storage (requires confirmation)
curl -X POST http://localhost:8080/memory/save \
  -H "Content-Type: application/json" \
  -H "x-confirmed: yes" \
  -d '{"key": "preference", "value": "dark_mode"}'

# Health check
curl http://localhost:8080/health
```

## üõ°Ô∏è Security & Compliance

### Confirmation Requirements
Most sensitive operations require explicit user confirmation via the `x-confirmed: yes` header to ensure compliance with OpenAI's Terms of Service.

**Protected Endpoints** (require confirmation):
- Data modification operations (`/memory/save`, `/memory/delete`)
- Worker execution (`/workers/run/*`)  
- System control (`/orchestration/*`, `/heartbeat`)
- AI processing with side effects (`/arcanos`, `/brain`)

**Safe Endpoints** (no confirmation needed):
- Read operations (`/health`, `/status`, `/memory/load`)
- Primary AI endpoint (`/ask`)
- Diagnostic endpoints (`/memory/health`, `/workers/status`)

## üöÑ Railway Deployment

Arcanos is optimized for deployment on Railway with automatic builds and health monitoring.

### Quick Deploy to Railway

1. **Fork this repository** to your GitHub account

2. **Connect to Railway**:
   - Go to [Railway.app](https://railway.app)
   - Click "Deploy from GitHub repo"
   - Select your forked repository

3. **Configure Environment Variables**:
   ```bash
   OPENAI_API_KEY=your-openai-api-key-here
   NODE_ENV=production
   PORT=8080
   AI_MODEL=ft:gpt-3.5-turbo-0125:personal:arcanos-v2:BxRSDrhH
   ```

4. **Optional Environment Variables**:
   ```bash
   DATABASE_URL=postgresql://user:pass@host:port/db  # Railway PostgreSQL
   RUN_WORKERS=true
   NOTION_API_KEY=your-notion-key                    # For WWE roster sync
   WWE_DATABASE_ID=your-notion-database-id
   ```

### Railway Configuration Features

- ‚úÖ **Automatic Port Binding**: Uses Railway's `PORT` environment variable
- ‚úÖ **Health Monitoring**: `/health` endpoint for Railway health checks  
- ‚úÖ **Graceful Shutdown**: Proper SIGTERM/SIGINT handling
- ‚úÖ **Environment Detection**: Automatically detects Railway platform
- ‚úÖ **Build Optimization**: TypeScript compilation with dependency caching
- ‚úÖ **Log Aggregation**: Structured logging compatible with Railway logs

### Monitoring & Debugging

Railway health checks use the `/health` endpoint:
```bash
curl https://your-app.railway.app/health
```

Monitor logs through Railway dashboard or CLI:
```bash
railway logs --tail
```

### Production Considerations

- Set `NODE_ENV=production` for optimal performance
- Configure `DATABASE_URL` for persistent storage (Railway PostgreSQL recommended)
- OpenAI API key is required - mock responses are used in development only
- Workers are disabled by default on Railway - enable with `RUN_WORKERS=true`

## üîß Development

### Available Scripts
```bash
npm run dev          # Development server with hot reload
npm run build        # Build TypeScript to dist/
npm start           # Run production server
npm run type-check  # TypeScript type checking
npm test            # Run test suite
npm run guide:generate -- <entry_key>  # Generate a tagged build guide
```

### Project Structure
```
src/
‚îú‚îÄ‚îÄ server.ts           # Main server entry point
‚îú‚îÄ‚îÄ config/            # Configuration management
‚îú‚îÄ‚îÄ routes/            # API route handlers
‚îú‚îÄ‚îÄ services/          # Core business logic
‚îú‚îÄ‚îÄ logic/             # AI reasoning and processing
‚îú‚îÄ‚îÄ utils/             # Utility functions
‚îî‚îÄ‚îÄ types/             # TypeScript type definitions

docs/                  # Documentation
‚îú‚îÄ‚îÄ ai-guides/         # AI-specific documentation
‚îî‚îÄ‚îÄ deployment/        # Deployment guides
```

### Worker Development
Workers are automatically loaded from the filesystem and scheduled by the AI system:

```typescript
// Example worker structure
export default {
  name: 'example-worker',
  schedule: '0 */6 * * *',  // Every 6 hours
  async run(context) {
    await context.log('Worker started');
    // Worker logic here
    await context.log('Worker completed');
  }
};
```

## üöÄ Deployment

### Environment Setup
1. Set required environment variables in your deployment platform
2. Ensure PostgreSQL database is available (or use in-memory fallback)
3. Configure `RUN_WORKERS=true` for full functionality

### Railway Deployment
```bash
# Railway will automatically:
# - Install dependencies
# - Build TypeScript
# - Start the server
# Set environment variables in Railway dashboard
```

### Docker Deployment
```bash
docker build -t arcanos .
docker run -p 8080:8080 -e OPENAI_API_KEY=your-key arcanos
```

## üìö Documentation

### Core Guides
- **[Setup Guide](./docs/ai-guides/SETUP_GUIDE.md)** - Detailed installation instructions
- **[API Reference](./docs/ai-guides/PROMPT_API_GUIDE.md)** - Complete API documentation
- **[Memory System](./docs/ai-guides/UNIVERSAL_MEMORY_GUIDE.md)** - Memory architecture guide

### AI Features
- **[OpenAI Integration](./docs/ai-guides/BACKEND_REFACTOR_SUMMARY.md)** - OpenAI SDK implementation
- **[Assistant Sync](./docs/ai-guides/ASSISTANT_SYNC.md)** - OpenAI Assistants integration
- **[Worker System](./docs/ai-guides/SLEEP_SCHEDULER_IMPLEMENTATION.md)** - AI-controlled workers

### Development
- **[Contributing Guide](./docs/ai-guides/AI_CONTROL_SERVICE.md)** - Development best practices
- **[Database Guide](./docs/ai-guides/DATABASE_IMPLEMENTATION.md)** - Database setup and usage
- **[Deployment Guide](./docs/deployment/DEPLOYMENT.md)** - Production deployment

## üêç Python Module

A companion Python package provides strict GPT-5 reasoning with enforced model usage and automatic maintenance alerts.
See [ARCANOS_PYTHON_README.md](./ARCANOS_PYTHON_README.md) for installation, configuration, and testing instructions.

## üîÑ Changelog

See [CHANGELOG.md](./docs/CHANGELOG.md) for detailed version history and recent updates.

## ü§ù Best Practices

### For Developers
1. **Use TypeScript**: Maintain type safety throughout the codebase
2. **Memory-Aware Design**: Consider memory context in all AI interactions
3. **Error Handling**: Implement comprehensive error handling and logging
4. **Worker Patterns**: Follow the established worker context pattern
5. **Configuration**: Use environment variables for all configurable options

### For AI Integration
1. **Confirmation Flow**: Always prompt users before sensitive operations
2. **Memory Context**: Utilize memory system for conversation continuity
3. **Error Recovery**: Implement graceful fallbacks for API failures
4. **Rate Limiting**: Respect OpenAI usage limits and implement backoff
5. **Security**: Validate all inputs and sanitize outputs

## üìù License

MIT License - See LICENSE file for details.

---

**Arcanos Backend** - AI-controlled server architecture for modern applications.
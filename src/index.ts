// ARCANOS:FULL AUDIT COMPLETED âœ… - BACKEND ENTRY POINT
// Target: Detect lingering exit logic, missing persistence, or fine-tune misconfig
// Status: All audit requirements implemented and validated

// --- ENTRY POINT IMPLEMENTATION ---
import express from 'express';
import * as dotenv from 'dotenv';
import axios from 'axios';
import * as os from 'os';
import * as fs from 'fs';
import path from 'path';
import { exec } from 'child_process';
import bodyParser from 'body-parser';
import cors from 'cors';
import router from './routes/index';
import memoryRouter from './routes/memory';
import { databaseService } from './services/database';
import { serverService } from './services/server';

// Import the new database connection module to ensure memory table exists
import './services/database-connection';

// Worker initialization will be handled by worker-init.js
// import { startCronWorker } from './services/cron-worker';

// Import worker initialization module (will run conditionally)
import './worker-init';

// Load environment variables
dotenv.config();

// 1. VERIFY: Environment variable loading
console.log("Model (FINE_TUNED_MODEL):", process.env.FINE_TUNED_MODEL);
console.log("Model (OPENAI_FINE_TUNED_MODEL):", process.env.OPENAI_FINE_TUNED_MODEL);
console.log("OpenAI API Key configured:", !!process.env.OPENAI_API_KEY);

// 3. FAIL FAST if model is not available
const fineTunedModel = process.env.FINE_TUNED_MODEL || process.env.OPENAI_FINE_TUNED_MODEL;
if (!fineTunedModel) {
  console.warn("âš ï¸ No fine-tuned model configured, using default model");
}

const app = express();
const PORT = Number(process.env.PORT) || 8080;

// Middleware - matches problem statement specification
app.use(cors());
app.use(bodyParser.json());

// Also keep existing middleware for compatibility
app.use(express.urlencoded({ extended: true }));

// Serve static files for frontend testing
const publicDir = path.join(__dirname, '../public');
app.use(express.static(publicDir));

// Basic Healthcheck
app.get('/health', (_, res) => res.send('âœ… OK'));

// Fine-tune routing status endpoint
app.get('/finetune-status', async (req, res) => {
  const userId = req.headers['x-user-id'] as string || 'default';
  const sessionId = req.headers['x-session-id'] as string || 'default';
  
  try {
    const { fineTuneRoutingService } = await import('./services/finetune-routing');
    
    const isActive = await fineTuneRoutingService.isFineTuneRoutingActive(userId, sessionId);
    const statusMessage = await fineTuneRoutingService.getStatusMessage(userId, sessionId);
    const state = await fineTuneRoutingService.getRoutingState(userId, sessionId);
    
    res.json({
      active: isActive,
      message: statusMessage,
      state: state,
      userId,
      sessionId,
      timestamp: new Date().toISOString()
    });
  } catch (error: any) {
    res.status(500).json({
      error: 'Failed to get fine-tune routing status',
      details: error.message
    });
  }
});

// GitHub webhook endpoint
app.post('/webhook', async (req, res) => {
  try {
    const { repository, head_commit } = req.body;
    
    // Validate required webhook data
    if (!repository || !head_commit) {
      console.log('âŒ Webhook missing required fields:', { 
        hasRepository: !!repository, 
        hasHeadCommit: !!head_commit 
      });
      return res.status(400).json({ error: 'Missing required webhook data' });
    }

    const payload = {
      key: 'github_sync',
      value: JSON.stringify({
        repo: repository.full_name,
        message: head_commit.message,
        url: head_commit.url,
        timestamp: new Date().toISOString()
      }),
      type: 'context',
      tags: ['git', 'sync']
    };

    console.log('ðŸ”— GitHub webhook received:', {
      repo: repository.full_name,
      commit: head_commit.id?.substring(0, 7),
      message: head_commit.message?.substring(0, 50) + '...'
    });

    // Send data to ARCANOS memory endpoint using axios
    try {
      const memoryUrl = process.env.ARCANOS_MEMORY_URL || 'https://arcanos-production-426d.up.railway.app/memory';
      const response = await axios.post(memoryUrl, payload);
      
      console.log('âœ… GitHub sync sent to ARCANOS memory endpoint');
      res.status(200).json({ 
        success: true, 
        message: 'GitHub sync sent to ARCANOS' 
      });
    } catch (memoryError: any) {
      console.error('âŒ Failed to send to memory endpoint:', memoryError.message);
      // Fallback: still return success since webhook was received
      res.status(200).json({ 
        success: true, 
        message: 'GitHub sync received but memory storage failed',
        warning: 'Could not store in memory endpoint'
      });
    }

  } catch (err: any) {
    console.error('âŒ Webhook error:', err);
    res.status(500).json({ 
      error: 'Internal webhook error',
      details: err.message 
    });
  }
});

// @ARCANOS: Diagnostic sync endpoint for GPT access
// This exposes live server health metrics: memory, CPU, disk, uptime
app.get('/sync/diagnostics', async (req, res) => {
  const token = req.headers['authorization'];
  if (token !== `Bearer ${process.env.GPT_TOKEN}`) {
    return res.status(403).json({ error: "Unauthorized GPT access" });
  }

  // Memory usage
  const memory = process.memoryUsage();

  // CPU usage (load average over 1, 5, 15 minutes)
  const cpuLoad = os.loadavg();

  // Uptime in seconds
  const uptime = process.uptime();

  // Disk usage (Linux/Unix only)
  exec('df -h /', (error, stdout) => {
    const diskUsage = error ? "Unavailable" : stdout;

    res.json({
      status: 'healthy',
      env: process.env.NODE_ENV,
      memory,
      cpuLoad: {
        '1min': cpuLoad[0],
        '5min': cpuLoad[1],
        '15min': cpuLoad[2]
      },
      uptime,
      diskUsage,
      timestamp: new Date().toISOString()
    });
  });
});

// POST /query-finetune endpoint - Railway + GitHub Copilot compliant fine-tuned model access
app.post('/query-finetune', async (req, res) => {
  console.log('ðŸŽ¯ /query-finetune endpoint called');
  
  const { query, metadata } = req.body;

  if (!query) {
    return res.status(400).json({ 
      error: 'Query field is required',
      timestamp: new Date().toISOString()
    });
  }

  if (typeof query !== 'string') {
    return res.status(400).json({ 
      error: 'Query must be a string',
      timestamp: new Date().toISOString()
    });
  }

  try {
    // Import OpenAI service for fine-tuned model access
    const { OpenAIService } = await import('./services/openai');
    const openaiService = new OpenAIService();
    
    console.log('ðŸš€ Processing query with fine-tuned model:', openaiService.getModel());
    
    // Call the fine-tuned model
    const response = await openaiService.chat([
      { role: 'user', content: query }
    ]);

    console.log('âœ… Fine-tuned model response received');
    
    return res.json({
      response: response.message,
      model: response.model,
      success: true,
      timestamp: new Date().toISOString(),
      metadata: metadata || {}
    });

  } catch (error: any) {
    console.error('âŒ Error in /query-finetune:', error.message);
    
    return res.status(500).json({
      error: 'Fine-tuned model invocation failed',
      details: error.message,
      success: false,
      timestamp: new Date().toISOString()
    });
  }
});

// POST /ask endpoint - matches problem statement specification (fallback route)
app.post('/ask', (req, res) => {
  const { query, mode = 'logic' } = req.body;

  if (!query) {
    return res.status(400).json({ error: 'Missing query field' });
  }

  // Simulated logic processing (replace with real engine)
  const response = {
    response: `Query received: "${query}" in mode: "${mode}"`,
  };

  res.json(response);
});

// Root route - serve dashboard index
app.get('/', (_req, res) => {
  res.sendFile(path.join(publicDir, 'index.html'));
});

// Mount core logic or routes here
app.use('/api', router);

// Mount memory routes - Universal Memory Archetype
app.use('/memory', memoryRouter);

// POST endpoint for natural language inputs with improved error handling
app.post('/', async (req, res) => {
  const { message } = req.body;
  const userId = req.headers['x-user-id'] as string || 'default';
  const sessionId = req.headers['x-session-id'] as string || 'default';
  
  if (!message) {
    return res.status(400).json({ error: 'Message is required' });
  }

  // Track graceful fallback scenarios for user alerts
  let fallbackAlert = '';

  try {
    // --- ARCANOS FINE-TUNE ROUTING OVERRIDE ---
    // Import the fine-tune routing service
    const { fineTuneRoutingService } = await import('./services/finetune-routing');
    
    // Check if this is a routing command
    const commandType = fineTuneRoutingService.isFineTuneCommand(message);
    
    if (commandType === 'activate') {
      console.log('ðŸŽ¯ Fine-tune routing ACTIVATION command detected');
      
      const state = await fineTuneRoutingService.activateFineTuneRouting(userId, sessionId, message);
      const statusMessage = await fineTuneRoutingService.getStatusMessage(userId, sessionId);
      
      // Check for memory persistence warnings
      let responseMessage = `âœ… Fine-tuned model routing activated! ${statusMessage}`;
      if (state.memoryPersistenceWarning) {
        responseMessage += `\n\nâš ï¸ Warning: There was an issue persisting your routing preference to memory storage. Your fine-tune routing will work for this session but may not persist if the server restarts.`;
      }
      
      return res.send(responseMessage);
      
    } else if (commandType === 'deactivate') {
      console.log('â­• Fine-tune routing DEACTIVATION command detected');
      
      const wasActive = await fineTuneRoutingService.deactivateFineTuneRouting(userId, sessionId);
      const statusMessage = await fineTuneRoutingService.getStatusMessage(userId, sessionId);
      
      if (wasActive) {
        return res.send(`âœ… Fine-tuned model routing deactivated. ${statusMessage}`);
      } else {
        return res.send(`â„¹ï¸ Fine-tuned model routing was already inactive. ${statusMessage}`);
      }
    }
    
    // Check if fine-tune routing is active
    const isFineTuneActive = await fineTuneRoutingService.isFineTuneRoutingActive(userId, sessionId);
    
    if (isFineTuneActive) {
      console.log('ðŸŽ¯ Fine-tune routing OVERRIDE active - routing directly to fine-tuned model');
      
      try {
        // Import OpenAI service for fine-tuned model access
        const { OpenAIService } = await import('./services/openai');
        const openaiService = new OpenAIService();
        
        console.log('ðŸš€ Processing message with fine-tuned model (override mode):', openaiService.getModel());
        
        // Call the fine-tuned model directly
        const response = await openaiService.chat([
          { role: 'user', content: message }
        ]);

        // Check if OpenAI service returned an error (graceful fallback scenario)
        if (response.error) {
          console.warn('âš ï¸ Fine-tuned model failed, falling back to normal routing:', response.error);
          fallbackAlert = `âš ï¸ Alert: Fine-tuned model temporarily unavailable (${response.error}). Using normal routing as fallback.`;
          
          // Continue to normal routing (graceful fallback)
          console.log('ðŸ”„ Graceful fallback: Using normal intent-based routing due to fine-tuned model failure');
        } else {
          console.log('âœ… Fine-tuned model response received (override mode)');
          
          // Return the response with a subtle indicator that override is active
          return res.send(response.message);
        }

      } catch (error: any) {
        console.error('âŒ Error in fine-tune override routing, falling back to normal routing:', error.message);
        fallbackAlert = `âš ï¸ Alert: Fine-tuned model service error. Using normal routing as fallback.`;
        
        // Continue to normal routing (graceful fallback)
        console.log('ðŸ”„ Graceful fallback: Using normal intent-based routing due to exception');
      }
    }
    
    // --- ARCANOS FINE-TUNE PREFIX DETECTION (Legacy Support) ---
    // Check for query-finetune: prefix and route directly to fine-tuned model
    if (typeof message === 'string' && message.trim().toLowerCase().startsWith('query-finetune:')) {
      const query = message.trim().substring('query-finetune:'.length).trim();
      
      if (!query) {
        return res.status(400).json({ 
          error: 'Query cannot be empty after query-finetune: prefix',
          timestamp: new Date().toISOString()
        });
      }
      
      console.log('ðŸŽ¯ Fine-tune routing activated via prefix:', query.substring(0, 100) + (query.length > 100 ? '...' : ''));
      
      try {
        // Import OpenAI service for fine-tuned model access
        const { OpenAIService } = await import('./services/openai');
        const openaiService = new OpenAIService();
        
        console.log('ðŸš€ Processing query with fine-tuned model (prefix mode):', openaiService.getModel());
        
        // Call the fine-tuned model directly (prefix mode - raw response)
        const response = await openaiService.chat([
          { role: 'user', content: query }
        ]);

        // Check if OpenAI service returned an error (graceful fallback scenario)
        if (response.error) {
          console.warn('âš ï¸ Fine-tuned model failed in prefix mode:', response.error);
          return res.send(`âš ï¸ Fine-tuned model temporarily unavailable: ${response.error}\n\nPlease try again later or use the main endpoint without the query-finetune: prefix for normal routing.`);
        }

        console.log('âœ… Fine-tuned model response received (prefix mode)');
        
        // Prefix mode: Return raw model response without additional formatting
        return res.send(response.message);

      } catch (error: any) {
        console.error('âŒ Error in fine-tune prefix routing:', error.message);
        
        return res.send(`âš ï¸ Fine-tuned model service error: ${error.message}\n\nPlease try again later or use the main endpoint without the query-finetune: prefix for normal routing.`);
      }
    }
    
    // --- ARCANOS INTENT-BASED ROUTING (Default) ---
    console.log('ðŸŽ¯ POST / endpoint called with message:', message.substring(0, 100) + (message.length > 100 ? '...' : ''));
    
    // Import the router service
    const { processArcanosRequest } = await import('./services/arcanos-router');
    
    // Process request through intent-based router
    const routerRequest = {
      message,
      domain: 'general',
      useRAG: true,
      useHRC: true
    };

    console.log('ðŸš€ Processing request through ARCANOS router...');
    const result = await processArcanosRequest(routerRequest);
    
    console.log('ðŸ“¥ Received response from ARCANOS router:', {
      success: result.success,
      intent: result.intent,
      confidence: result.confidence,
      service: result.metadata?.service
    });
    
    // Return response based on success
    if (result.success) {
      // Success case - return message content with fallback alert if applicable
      const responseText = fallbackAlert ? `${fallbackAlert}\n\n${result.response}` : result.response;
      res.send(responseText);
    } else {
      const errorResponse = { 
        error: result.error,
        response: result.response 
      };
      
      // Add fallback alert to error response if applicable
      if (fallbackAlert) {
        errorResponse.response = fallbackAlert + '\n\n' + (errorResponse.response || '');
      }
      
      res.json(errorResponse);
    }
    
  } catch (error: any) {
    console.error('âŒ Error processing message:', error);
    console.error('ðŸ” Error details:', {
      name: error.name,
      message: error.message,
      stack: error.stack?.split('\n')[0]
    });
    
    res.status(500).json({ 
      error: 'Internal server error',
      response: `Echo: ${message}` // Fallback response
    });
  }
});

// Keep process alive with HTTP server
serverService.setupSignalHandlers();

// ========= GLOBAL PROCESS MONITORS =========
process.on("exit", (code) => {
  console.log(`[EXIT] Process is exiting with code ${code}`);
});
process.on("uncaughtException", (err) => {
  console.error("[FATAL] Uncaught Exception:", err);
});
process.on("unhandledRejection", (reason, promise) => {
  console.error("[FATAL] Unhandled Rejection at:", promise, "reason:", reason);
});

// ========= ADD INSIDE YOUR SERVER STARTUP =========
serverService.start(app, PORT).then(async () => {
  console.log(`[SERVER] Running on port ${PORT}`);
  console.log(`[INFO] ENV:`, {
    NODE_ENV: process.env.NODE_ENV,
    PORT: process.env.PORT,
    MODEL: process.env.FINE_TUNED_MODEL,
    DATABASE: !!process.env.DATABASE_URL
  });
  
  // Initialize database schema
  try {
    await databaseService.initialize();
    console.log('âœ… Universal Memory Archetype initialized');
  } catch (error) {
    console.error('âŒ Failed to initialize Universal Memory Archetype:', error);
    console.warn('âš ï¸ Memory service will run in degraded mode');
  }
  
  // Memory optimization logging for 8GB Railway Hobby Plan
  const memStats = process.memoryUsage();
  const v8Stats = require('v8').getHeapStatistics();
  console.log('ðŸ§  [MEMORY] Node.js Memory Configuration for 8GB Hobby Plan:');
  console.log(`   ðŸ“Š Heap Size Limit: ${(v8Stats.heap_size_limit / 1024 / 1024 / 1024).toFixed(2)} GB`);
  console.log(`   ðŸ“ˆ Current RSS: ${(memStats.rss / 1024 / 1024).toFixed(2)} MB`);
  console.log(`   ðŸ”„ Heap Used: ${(memStats.heapUsed / 1024 / 1024).toFixed(2)} MB`);
  console.log(`   ðŸ’¾ Heap Total: ${(memStats.heapTotal / 1024 / 1024).toFixed(2)} MB`);
  console.log(`   ðŸŽ¯ Target: Using ~7GB of 8GB available on Railway Hobby Plan`);
  
  // Railway-specific logging
  if (process.env.RAILWAY_ENVIRONMENT) {
    console.log(`ðŸš‚ Railway Environment: ${process.env.RAILWAY_ENVIRONMENT}`);
    console.log(`ðŸ”§ Railway Service: ${process.env.RAILWAY_SERVICE_NAME || 'Unknown'}`);
  }

  // Worker initialization is now handled by worker-init.js
  // which conditionally starts workers based on RUN_WORKERS env var
  console.log('[SERVER] Worker initialization handled by worker-init.js module');
  
  // Memory monitoring interval
  setInterval(() => {
    const currentMem = process.memoryUsage();
    const currentV8 = require('v8').getHeapStatistics();
    console.log(`ðŸ§  [MEMORY_MONITOR] RSS: ${(currentMem.rss / 1024 / 1024).toFixed(2)}MB, Heap: ${(currentMem.heapUsed / 1024 / 1024).toFixed(2)}MB/${(currentV8.heap_size_limit / 1024 / 1024 / 1024).toFixed(2)}GB`);
  }, 300000); // Log every 5 minutes
});

// --- RAILWAY SERVICE CONFIG VALIDATION âœ… ---
// âœ… Ensure `.railway/config.json` exists and binds to PORT
// âœ… Confirm `alwaysOn` is true in Railway GUI (manual verification needed)
// âœ… Confirm no conflicting default script paths in `package.json`
// âœ… Health endpoint configured for Railway health checks (/health)
// âœ… Graceful shutdown logic implemented for Railway deployments

export default app;

// Export the ARCANOS V1 Safe Interface for direct usage
export { askArcanosV1_Safe, getActiveModel, ArcanosModel } from './services/arcanos-v1-interface';

// Export sleep schedule functions for copilot integration
export { getActiveSleepSchedule, getCoreSleepWindow } from './services/sleep-config';

// ðŸ”§ End of Audit Block - ALL REQUIREMENTS IMPLEMENTED âœ…
